@id Lfr366UIpxkQ0lWE
@title rebutting Searle's Chinese Box argument
@created 1521494018214
@text ```
Searle's Chinese Box argument is unconvincing to me.

First of all, it assumes the impossible: a room with paper instructions for how to respond to every conceivable set of questions. There are only 10^80 atoms in the observed universe. Pretend English only has 10K words, and pretend questions only have three words, and suppose the Chinese Box only has to be prepared for any series of 7 questions. Already there are more sequences of questions than there are atoms in the universe.

That might sound like a technicality, but it's critical. Searle's argument hinges on the idea that answering questions well does not require a thinking agent. The only kinds of agents we have ever seen answer questions well are intelligent, and yet somehow he thinks intelligence is unnecessary.

But suppose we go along with Searle's assumptions. His argument is that the homonculus running around looking up the answers to questions clearly doesn't know what it's doing. That's true. Neither do your neurons, the agents by which you calculate. When I say that you are conscious, I'm not saying that any particular part of you is conscious. I'm saying that the totaility of you is conscious. Similarly, no matter how dumb the homonculous is, the room is smart.

What, after all, is the difference between a room that answers quesitons and a person? The person uses meat to do its thinking? Granting the quality of intelligence only to things that look like you is the kind of thinking that justified the African slave trade.

No. The reason you think someone is intelligent is because of their behavior.
```
